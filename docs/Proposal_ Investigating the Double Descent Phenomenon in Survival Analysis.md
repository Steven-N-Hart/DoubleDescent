# **Title: The "Survival Double Descent": Stress-Testing Deep Learning Generalization in Time-to-Event Analysis Using Synthetic Data**

## **1\. Abstract**

The "Double Descent" phenomenon—where generalization error decreases, peaks at the interpolation threshold ($p \\approx n$), and then decreases again in the over-parameterized regime ($p \\gg n$)—has challenged classical statistical wisdom in computer vision and NLP. However, its manifestation in **Survival Analysis** remains under-explored. Survival models differ fundamentally due to censoring and the usage of rank-based loss functions (e.g., Cox Partial Likelihood) which may not behave like Mean Squared Error. This study proposes a rigorous simulation-based framework to detect and characterize double descent in deep survival models. We will stress-test the phenomenon across four distinct data regimes: (1) Baseline Gaussian covariates, (2) Skewed (Log-Normal) covariates, (3) High-cardinality categorical covariates, and (4) Heavily censored (imbalanced) data. We hypothesize that the "Interpolation Peak" in survival analysis is driven by the number of *events* ($N\_{events}$) rather than total sample size ($N$), and that rank-based metrics (C-index) may mask overfitting catastrophes that calibration metrics (Brier Score) reveal.

## ---

**2\. Introduction & Background**

### **2.1 The Double Descent Paradox**

Classical statistical learning theory advocates for parsimony, warning that models with more parameters ($p$) than data points ($n$) will overfit. Modern deep learning contradicts this, often achieving optimal performance in the highly over-parameterized regime. This behavior is described by the **Double Descent** curve:

1. **Classical Regime:** Error drops as complexity increases.  
2. **Critical Regime ($p \\approx n$):** Error spikes as the model forces interpolation of noise (variance explosion).  
3. **Modern Regime ($p \\gg n$):** Error drops again as the optimization algorithm (SGD) finds a "smooth" interpolating solution.1

### **2.2 The Gap in Survival Analysis**

While documented in classification/regression, this phenomenon is not well-characterized in Time-to-Event (TTE) models. Recent theoretical work suggests that survival loss functions (like Cox Partial Likelihood) may exhibit unique "finite-norm interpolation" properties that alter the shape of the curve.3 Furthermore, clinical data often violates the standard assumptions of "clean" double descent papers (e.g., Gaussian features, balanced classes) by containing skewed biomarkers, sparse categorical codes, and heavy censoring.

## ---

**3\. Research Objectives**

1. **Detection:** Confirm if the double descent peak occurs in standard Deep Survival models (e.g., DeepSurv) and if it aligns with the number of observations ($N$) or the number of events ($N\_{events}$).  
2. **Distributional Sensitivity:** Determine if **skewed distributions** (common in medical costs/biomarkers) and **high-cardinality** variables exacerbate the variance peak at the interpolation threshold.  
3. **Metric Divergence:** Test the hypothesis that **Concordance Index (C-index)** is blind to double descent (due to rank-invariance), while **Integrated Brier Score (IBS)** reveals the true calibration failure in the critical regime.

## ---

**4\. Methodology: Synthetic Data Framework**

To isolate these phenomena, we will use a **generative simulation approach**. Real-world data prevents the precise control of the signal-to-noise ratio and distribution types required to map the full curve.

### **4.1 The Generative Engine (Inverse Transform Sampling)**

We will generate survival times $T$ conditional on covariates $X$ using a proportional hazards framework with a Weibull baseline hazard.4

Step 1: Generate Covariates ($X$)  
We will implement four scenarios:

* **Scenario A (Baseline):** $X \\sim \\mathcal{N}(0, 1)$ (Standard Normal).  
* **Scenario B (Skewed):** $X \\sim \\text{LogNormal}(0, 1)$. This mimics biomarkers like CRP or D-dimer.  
* **Scenario C (High Cardinality):** $X$ includes categorical variables with 100+ levels (e.g., simulating zip codes or ICD codes), encoded via One-Hot or Embeddings.  
* **Scenario D (Imbalance):** $X \\sim \\mathcal{N}(0, 1)$, but we vary the censoring mechanism to achieve 20%, 50%, and 90% censoring rates.

Step 2: Generate Event Times  
The hazard function for subject $i$ is:  
$$h(t|x\_i) \= \\lambda \\nu t^{\\nu-1} \\exp(\\beta^T x\_i)$$The survival time $T\_i$ is generated by inverting the survival function $S(t|x) \= U$, where $U \\sim \\text{Uniform}(0,1)$:

$$T\_i \= \\left( \\frac{-\\ln(U)}{\\lambda \\exp(\\beta^T x\_i)} \\right)^{1/\\nu}$$  
Step 3: Censoring Mechanism  
Censoring times $C\_i$ will be drawn from an exponential distribution $C\_i \\sim \\text{Exp}(\\lambda\_c)$. The parameter $\\lambda\_c$ will be tuned iteratively to hit exact censoring percentages (e.g., 90% censoring for the imbalance test).6

### **4.2 Model Architecture & Training**

* **Model:** **DeepSurv** (Multi-layer Perceptron optimizing Cox Partial Likelihood).  
* **Capacity Control:** We will vary the **width** ($k$) of the hidden layers.  
* **Protocol:**  
  1. Fix sample size $N \= 1000$.  
  2. Train models with widths $k \\in \[2, 4,..., 2000\]$.  
  3. This sweeps the parameter count $P$ from $P \\ll N$ (under-parameterized) to $P \\gg N$ (over-parameterized).  
  4. **Crucial:** No early stopping or regularization (weight decay \= 0\) will be used initially, as these dampen the double descent effect.1

## ---

**5\. Experiments and Anticipated Results**

### **Experiment 1: The "Effective Sample Size" Hypothesis**

* **Setup:** Compare the Double Descent curve for a dataset with 0% censoring vs. 80% censoring ($N=1000$ in both).  
* **Hypothesis:** The interpolation peak (spike in test error) will shift left in the censored dataset.  
* **Rationale:** The "effective" information in survival analysis is driven by the number of events ($E$). We expect the peak to occur near $P \\approx E$, not $P \\approx N$. This would be a novel finding for clinical deep learning guidance.

### **Experiment 2: The "Skewness" Stress Test**

* **Setup:** Train on Scenario B (Log-Normal inputs) vs. Scenario A (Normal inputs).  
* **Hypothesis:** The peak in Brier Score will be significantly higher and wider for the skewed data.  
* **Rationale:** Models struggle to interpolate "outliers" (tail values in log-normal data). In the critical regime ($P \\approx N$), the model weights must explode to fit these tail samples, causing massive generalization error.7

### **Experiment 3: Metric Divergence (The "Hidden" Danger)**

* **Setup:** Plot **C-index** and **Integrated Brier Score (IBS)** side-by-side for all experiments.  
* **Hypothesis:** The C-index will show a monotonic increase (or flatline), **failing to detect the double descent peak**. The IBS will show the catastrophic peak clearly.  
* **Significance:** This would prove that relying solely on C-index (standard practice in medical AI) is dangerous, as it selects models that are "discriminative" but statistically unstable and poorly calibrated.8

## ---

**6\. Proposed Python Implementation (Draft Snippet)**

Python

import numpy as np  
import pandas as pd  
from pysurvival.models.semi\_parametric import NonLinearCoxPHModel  
from sklearn.model\_selection import train\_test\_split  
from sklearn.metrics import brier\_score\_loss

def generate\_synthetic\_survival(N=1000, n\_features=20, skew=False, censor\_rate=0.3):  
    \# 1\. Covariates  
    if skew:  
        X \= np.random.lognormal(0, 1, (N, n\_features))  
    else:  
        X \= np.random.normal(0, 1, (N, n\_features))  
      
    \# 2\. True Weights (Sparse signal)  
    beta \= np.random.uniform(-1, 1, n\_features)  
    beta\[10:\] \= 0  \# Only first 10 features matter  
      
    \# 3\. Weibull Time Generation  
    lambda\_k, nu\_k \= 0.5, 2.0   
    u \= np.random.uniform(0, 1, N)  
    \# Inverse transform  
    linear\_pred \= np.dot(X, beta)  
    T \= (-np.log(u) / (lambda\_k \* np.exp(linear\_pred)))\*\*(1/nu\_k)  
      
    \# 4\. Censoring  
    C \= np.random.exponential(scale=np.mean(T)/censor\_rate, size=N)  
    time \= np.minimum(T, C)  
    event \= (T \<= C).astype(int)  
      
    return X, time, event

\# Experimental Loop Concept  
widths \=   
results \=

for w in widths:  
    \# Define model with specific width (Capacity control)  
    model \= NonLinearCoxPHModel(structure=)  
      
    \# Train to interpolation (0 loss or max epochs)  
    model.fit(X\_train, T\_train, E\_train, lr=1e-3, num\_epochs=10000, verbose=False)  
      
    \# Measure  
    c\_index \= model.concordance\_index(X\_test, T\_test, E\_test)  
    \# Calculate IBS (Pseudo-code)  
    ibs \= integrated\_brier\_score(model, X\_test, T\_test, E\_test)  
      
    results.append({'width': w, 'c\_index': c\_index, 'ibs': ibs})

## ---

**7\. References (BibTeX)**

Code snippet

@article{nakkiran2021deep,  
  title={Deep double descent: Where bigger models and more data hurt},  
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},  
  journal={Journal of Statistical Mechanics: Theory and Experiment},  
  volume={2021},  
  number={12},  
  pages={124003},  
  year={2021}  
}

@article{belkin2019reconciling,  
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},  
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},  
  journal={Proceedings of the National Academy of Sciences},  
  volume={116},  
  number={32},  
  pages={15849--15854},  
  year={2019}  
}

@article{liu2025understanding,  
  title={Understanding Overparametrization in Survival Models through Double-Descent},  
  author={Liu, Yin and Cai, Jianwen and Li, Didong},  
  journal={arXiv preprint arXiv:2512.12463},  
  year={2025}  
}

@article{bender2005generating,  
  title={Generating survival times to simulate Cox proportional hazards models},  
  author={Bender, Ralf and Augustin, Thomas and Blettner, Maria},  
  journal={Statistics in Medicine},  
  volume={24},  
  number={11},  
  pages={1713--1723},  
  year={2005}  
}

@article{austin2012generating,  
  title={Generating survival times to simulate Cox proportional hazards models with time-varying covariates},  
  author={Austin, Peter C},  
  journal={Statistics in Medicine},  
  volume={31},  
  number={29},  
  pages={3946--3958},  
  year={2012}  
}

@article{kvamme2019time,  
  title={Time-to-event prediction with neural networks and Cox regression},  
  author={Kvamme, H{\\aa}vard and Borgan, {\\O}rnulf and Scheel, Ida},  
  journal={Journal of Machine Learning Research},  
  volume={20},  
  number={129},  
  pages={1--30},  
  year={2019}  
}

@article{feldman2020neural,  
  title={What neural networks memorize and why: Discovering the long tail via influence estimation},  
  author={Feldman, Vitaly and Zhang, Chiyuan},  
  journal={arXiv preprint arXiv:2008.03703},  
  year={2020}  
}

@article{blanche2015quantifying,  
  title={Quantifying and comparing dynamic predictive accuracy of joint models for longitudinal marker and time-to-event in presence of censoring and competing risks},  
  author={Blanche, Paul and Proust-Lima, C{\\'e}cile and Jacqmin-Gadda, H{\\'e}l{\\\`e}ne},  
  journal={Biometrics},  
  volume={71},  
  number={1},  
  pages={102--113},  
  year={2015}  
}

@article{hartman2023pitfalls,  
  title={Pitfalls of the concordance index for survival outcomes},  
  author={Hartman, Nicholas and Kim, Sehee and He, Kevin and Kalbfleisch, John D},  
  journal={Statistics in Medicine},  
  volume={42},  
  number={13},  
  pages={2179--2190},  
  year={2023}  
}

#### **Works cited**

1. Understanding Overparametrization in Survival Models through Double-Descent \- arXiv, accessed January 4, 2026, [https://arxiv.org/html/2512.12463v1](https://arxiv.org/html/2512.12463v1)  
2. Deep double descent | OpenAI, accessed January 4, 2026, [https://openai.com/index/deep-double-descent/](https://openai.com/index/deep-double-descent/)  
3. Double descent \- Wikipedia, accessed January 4, 2026, [https://en.wikipedia.org/wiki/Double\_descent](https://en.wikipedia.org/wiki/Double_descent)  
4. Deep Double Descent: Where Bigger Models and More Data Hurt \- OpenReview, accessed January 4, 2026, [https://openreview.net/forum?id=B1g5sA4twr](https://openreview.net/forum?id=B1g5sA4twr)  
5. Characterizations of Double Descent \- SIAM.org, accessed January 4, 2026, [https://www.siam.org/publications/siam-news/articles/characterizations-of-double-descent/](https://www.siam.org/publications/siam-news/articles/characterizations-of-double-descent/)  
6. Exact expressions for double descent and implicit regularization via surrogate random design \- UC Berkeley Statistics, accessed January 4, 2026, [https://www.stat.berkeley.edu/\~mmahoney/pubs/NeurIPS-2020-double-descent.pdf](https://www.stat.berkeley.edu/~mmahoney/pubs/NeurIPS-2020-double-descent.pdf)  
7. \[2512.12463\] Understanding Overparametrization in Survival Models through Double-Descent \- arXiv, accessed January 4, 2026, [https://arxiv.org/abs/2512.12463](https://arxiv.org/abs/2512.12463)  
8. Understanding Overparametrization in Survival Models through Double-Descent, accessed January 4, 2026, [https://www.researchgate.net/publication/398719874\_Understanding\_Overparametrization\_in\_Survival\_Models\_through\_Double-Descent](https://www.researchgate.net/publication/398719874_Understanding_Overparametrization_in_Survival_Models_through_Double-Descent)  
9. Survival Analysis and Interpretation of Time-to-Event Data: The Tortoise and the Hare \- PMC, accessed January 4, 2026, [https://pmc.ncbi.nlm.nih.gov/articles/PMC6110618/](https://pmc.ncbi.nlm.nih.gov/articles/PMC6110618/)
\documentclass[12pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================

% Typography and Layout
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\doublespacing

% Mathematics
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}

% Graphics and Tables
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{float}

% References and Citations (author-year style for JASA)
\usepackage[authoryear,round]{natbib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Supplementary
\usepackage{xcolor}

% ============================================================================
% CUSTOM COMMANDS
% ============================================================================
\newcommand{\cindex}{C\text{-index}}
\newcommand{\ibs}{\text{IBS}}
\newcommand{\nll}{\text{NLL}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btheta}{\bm{\theta}}

% ============================================================================
% TITLE AND AUTHORS
% ============================================================================

\title{The Survival Double Descent: Generalization Dynamics of Deep Neural Networks in Time-to-Event Analysis}

\author{Steven N. Hart, PhD, ACHIP\thanks{Steven N. Hart is Associate Professor, Department of Laboratory Medicine and Pathology, and Department of Quantitative Health Sciences, Mayo Clinic, Rochester, MN 55905 (E-mail: hart.steven@mayo.edu).} \and Ann L. Oberg, PhD\thanks{Ann L. Oberg is Professor, Department of Quantitative Health Sciences, Mayo Clinic, Rochester, MN 55905.}}

\date{}

% ============================================================================
% DOCUMENT
% ============================================================================

\begin{document}

\maketitle

% ----------------------------------------------------------------------------
% ABSTRACT
% ----------------------------------------------------------------------------
\begin{abstract}
Double descent upends classical intuitions about overfitting. Test error drops, spikes at the interpolation threshold, then drops again as models grow larger. The phenomenon is well-documented in classification and regression. Survival analysis is different. Censoring hides true event times. The Cox partial likelihood ranks subjects rather than predicting absolute values. Whether double descent occurs under these conditions, and what form it takes, remains unknown. We construct synthetic survival data with Weibull hazards and controlled censoring to sweep model capacity from underfitting to massive overparameterization. Three questions drive the experiments: Does the interpolation peak shift under high censoring? Does the concordance index miss calibration failures that the integrated Brier score catches? Do log-normal covariates alter the pattern? The answers matter. Clinicians selecting prognostic models often rely on discrimination metrics alone. Our experiments reveal that concordance recovers in the overparameterized regime while integrated Brier score saturates at a constant value, indicating that discrimination-based model selection may miss calibration breakdown. L2 regularization attenuates the double descent peak by approximately 2\% absolute concordance.
\end{abstract}

\clearpage

% ----------------------------------------------------------------------------
% 1. INTRODUCTION
% ----------------------------------------------------------------------------
\section{Introduction}

The bias-variance trade-off has anchored statistical learning theory for decades \citep{hastie2009elements}. Increasing model complexity reduces bias while increasing variance; optimal generalization requires balancing these competing forces. This principle motivated the development of model selection criteria such as cross-validation and regularization methods \citep{tibshirani1996regression}.

Deep learning has challenged this framework. Networks with billions of parameters generalize well despite $p \gg n$ \citep{zhang2021understanding}. \citet{belkin2019reconciling} termed this the double descent phenomenon; \citet{nakkiran2021deep} documented it across architectures and datasets. The pattern proceeds in three stages. Small models underfit. At the interpolation threshold, where capacity just suffices to memorize training data, test error peaks sharply. The model fits noise exactly, but with a unique, highly oscillatory solution. Beyond this threshold, test error decreases again, often falling below the classical minimum. Gradient descent favors minimum-norm solutions when infinitely many interpolants exist \citep{bartlett2020benign}, and this implicit regularization enables generalization in overparameterized models.

Survival analysis remains outside this literature. Neural network methods for time-to-event data, including DeepSurv \citep{katzman2018deepsurv}, DeepHit \citep{lee2018deephit}, and related architectures, now appear routinely in clinical research \citep{wiegrebe2024deep}. However, no systematic investigation has established whether double descent occurs in this setting. \citet{liu2025understanding} provide preliminary theory suggesting it does, though possibly in modified form: the second descent may be attenuated, and benign overfitting, the phenomenon where interpolating models generalize well despite fitting training noise, may not fully materialize.

Several features of survival analysis may alter double descent dynamics. Censoring reduces available information: a patient lost to follow-up contributes only a lower bound on survival time, not a precise measurement. The Cox partial likelihood \citep{cox1972regression} optimizes rankings rather than predictions, and extreme risk scores ($\hat{\eta} \to \pm\infty$) maximize the likelihood for separable data. These properties produce an unusual loss geometry. Whether the interpolation threshold depends on total sample size $N$ or on the number of observed events $N_{\text{events}}$ remains an open question.

Evaluation metrics introduce additional complexity. The concordance index \citep{harrell1996multivariable} measures discrimination, quantifying a model's ability to rank patients by risk. It is invariant to monotonic transformations: a model predicting risk scores of $-1000$ and $+1000$ achieves perfect concordance if the ranking is correct. The integrated Brier score \citep{graf1999assessment} measures calibration and penalizes miscalibrated survival probabilities. These metrics may diverge at the interpolation threshold, with concordance remaining stable while calibration deteriorates. If clinicians select models based on discrimination alone (a common practice; see \citealt{hartman2023pitfalls}), they risk choosing overconfident, unstable predictors.

This paper addresses four questions. First, does double descent occur in deep Cox models? Second, is the interpolation peak governed by $N$ or $N_{\text{events}}$? Third, do discrimination and calibration metrics diverge near the threshold? Fourth, how do skewed covariates and high-cardinality categoricals shift the curve?

We investigate these questions through simulation. Synthetic data permit systematic capacity sweeps, precise noise control, and verification against known ground truth. The experiments span Gaussian and log-normal covariates, low and extreme censoring rates, and categorical features with up to 100 levels. Models are trained without regularization to expose the double descent curve; parallel experiments with weight decay characterize how standard regularization modifies the pattern.

These questions have practical implications. Survival models inform treatment decisions, resource allocation, and patient counseling \citep{harrell2015regression}. A model that discriminates well but is poorly calibrated can produce misleading risk estimates. Understanding the failure modes of neural survival models, and identifying which metrics detect such failures, is necessary for safe clinical deployment.

% ----------------------------------------------------------------------------
% 2. BACKGROUND
% ----------------------------------------------------------------------------
\section{Background}

\subsection{Double Descent Mechanics}

Consider standard regression with target $y = f^*(\bx) + \epsilon$ and noise variance $\sigma^2$. The expected test error decomposes as
\begin{equation}
    \E[(y - \hat{f}(\bx))^2] = \text{Bias}^2[\hat{f}(\bx)] + \text{Var}[\hat{f}(\bx)] + \sigma^2.
\end{equation}
Classical theory predicts that bias decreases with model capacity while variance increases, with optimal complexity achieving the minimum of their sum.

The interpolation threshold disrupts this picture \citep{belkin2019reconciling}. When the number of parameters equals the number of effective constraints, a unique solution interpolates the training data. This solution must pass through every training point, including noise, producing high variance and a peak in test error.

Beyond this threshold, infinitely many interpolating solutions exist. Gradient descent from small initialization converges to the minimum-norm solution \citep{bartlett2020benign}. Smaller weights correspond to smoother functions, and test error decreases.

\subsection{Survival Analysis Setup}

For each subject $i$, we observe $(Y_i, \delta_i)$ where $Y_i = \min(T_i, C_i)$ is the observed time and $\delta_i = \mathbf{1}(T_i \leq C_i)$ is the event indicator. Here $T_i$ denotes the true event time and $C_i$ the censoring time. Censored observations ($\delta_i = 0$) provide only the constraint $T_i > C_i$.

The Cox proportional hazards model \citep{cox1972regression} specifies the hazard function as $h(t | \bx_i) = h_0(t) \exp(\eta_i)$, where $\eta_i = f(\bx_i)$ is the log-risk score and $h_0(t)$ is an unspecified baseline hazard. DeepSurv \citep{katzman2018deepsurv} parameterizes $f$ using a neural network and estimates parameters by maximizing the partial likelihood:
\begin{equation}
    \mathcal{L}(\btheta) = \prod_{i: \delta_i = 1} \frac{\exp(\eta_i)}{\sum_{j \in \mathcal{R}_i} \exp(\eta_j)},
\end{equation}
where $\mathcal{R}_i = \{j : Y_j \geq Y_i\}$ is the risk set at time $Y_i$.

Three properties of the Cox partial likelihood are relevant to double descent \citep{liu2025understanding}. The likelihood depends only on the ranking of risk scores within risk sets, not on their magnitudes. For separable data, the optimal weights diverge to $\pm\infty$. Censored observations contribute to risk sets but not to the likelihood product, reducing the effective sample size.

\subsection{Evaluation Metrics}

The concordance index \citep{harrell1996multivariable} is defined as
\begin{equation}
    C = P(\hat{\eta}_i > \hat{\eta}_j \mid T_i < T_j).
\end{equation}
This is a ranking metric, invariant to monotonic transformations of $\hat{\eta}$. Extreme but correctly-ordered predictions achieve high concordance.

The Brier score at time $t$ \citep{graf1999assessment} is defined as
\begin{equation}
    \text{BS}(t) = \frac{1}{n} \sum_{i=1}^{n} \left[ \hat{S}(t | \bx_i) - \mathbf{1}(Y_i > t) \right]^2 \cdot w_i(t),
\end{equation}
where $w_i(t)$ are inverse probability of censoring weights \citep{gerds2006consistent}. The integrated Brier score averages over a time interval: $\text{IBS} = t_{\max}^{-1} \int_0^{t_{\max}} \text{BS}(t) \, dt$. Unlike concordance, the Brier score penalizes miscalibrated probability estimates.

At the interpolation threshold, we hypothesize that concordance remains stable because correct rankings may persist even as risk score magnitudes become extreme. The integrated Brier score, however, should increase due to miscalibrated survival curves. Model selection based solely on concordance would fail to detect this deterioration.

% ----------------------------------------------------------------------------
% 3. METHODS
% ----------------------------------------------------------------------------
\section{Methods}

\subsection{Data Generation}

Real datasets lack the controlled conditions required to trace the double descent curve. We therefore generate synthetic survival data.

Covariates are generated using a Gaussian copula. We draw $\bm{Z} \sim \N(\bm{0}, \bm{\Sigma})$ and transform marginals as follows: identity for Gaussian, exponentiation for log-normal, and quantile binning for categorical variables.

Event times follow a Weibull-Cox model:
\begin{equation}
    h(t | \bx_i) = \lambda \nu t^{\nu - 1} \exp(\bbeta^\top \bx_i).
\end{equation}
Inverse transform sampling yields
\begin{equation}
    T_i = \left( \frac{-\ln(U)}{\lambda \exp(\bbeta^\top \bx_i)} \right)^{1/\nu}, \quad U \sim \text{Uniform}(0, 1).
\end{equation}
Ground truth is known exactly \citep{bender2005generating}.

Censoring times follow an exponential distribution with rate $\lambda_c$ calibrated to achieve target censoring proportions. The observed data are $Y_i = \min(T_i, C_i)$ and $\delta_i = \mathbf{1}(T_i \leq C_i)$.

\subsection{Scenarios}

We consider four configurations (Table~\ref{tab:scenarios}). Scenario A uses Gaussian covariates with 30\% censoring as a baseline. Scenario B uses log-normal covariates to test whether leverage points amplify the interpolation peak. Scenario C includes five categorical features with 100 levels each to test threshold shifts from one-hot encoding. Scenario D uses 90\% censoring to test whether the peak location depends on $N_{\text{events}}$ rather than $N$.

\begin{table}[ht]
\centering
\caption{Experimental scenarios.}
\label{tab:scenarios}
\begin{tabular}{llll}
\toprule
\textbf{Scenario} & \textbf{Covariates} & \textbf{Specification} & \textbf{Target} \\
\midrule
A & Gaussian & $X \sim \N(0, I)$, 30\% censoring & Baseline curve \\
B & Log-normal & $X \sim \text{LogNormal}(0, 1)$ & Peak amplitude \\
C & Categorical & 5 features, $K=100$ levels & Threshold location \\
D & Gaussian & 90\% censoring & Effective $N$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Models and Training}

The network architecture is a multi-layer perceptron with fixed depth and variable width: two hidden layers, each of width $w$, with ReLU activations, followed by a single linear output node. The model is trained to minimize negative Cox partial log-likelihood.

To observe the double descent phenomenon without confounding effects, we train without explicit regularization: the Adam optimizer runs for 10,000 epochs with batch size 256 and learning rate 0.001, with no early stopping, weight decay, or dropout. Parallel experiments include weight decay ($\lambda = 0.01$) to characterize how regularization modifies the curve.

We hold depth constant at two hidden layers and sweep width over $w \in \{2, 4, \ldots, 2048\}$ in powers of two, producing parameter counts spanning approximately $0.1N$ to $100N$ for $N = 1000$ samples.

\subsection{Evaluation}

Data are partitioned into training (60\%), validation (20\%), and test (20\%) sets. We compute concordance index, integrated Brier score, and negative log partial likelihood on the held-out test set.

Each configuration is replicated across multiple random seeds. We report means and standard deviations, the location of the interpolation peak, peak magnitude relative to the classical minimum, and the divergence between concordance and integrated Brier score at the threshold.

% ----------------------------------------------------------------------------
% 4. RESULTS
% ----------------------------------------------------------------------------
\section{Results}

\subsection{Baseline Double Descent Curve}

Figure~\ref{fig:double_descent} displays the concordance index as a function of model capacity for Scenario A (Gaussian covariates, 30\% censoring), aggregated across five random seeds. Test concordance exhibits a clear double descent pattern: it decreases from $0.828 \pm 0.015$ at $w = 2$ to a minimum of $0.737 \pm 0.027$ at $w = 16$, then recovers to $0.804 \pm 0.015$ at $w = 2048$. The shaded region represents one standard deviation across seeds.

The vertical dashed line marks the interpolation threshold, estimated at $P \approx N$ where the number of parameters equals the training sample size. The test error peak (minimum concordance) occurs at width $w = 16$, where parameter count (625) approaches the number of training samples ($N = 600$). The star marker highlights this critical point.

The pattern confirms double descent occurs in survival analysis, though with notable differences from classification. The concordance recovery in the overparameterized regime ($w > 64$) is incomplete: at $w = 2048$, concordance reaches 0.804 but does not return to the baseline of 0.828 observed at $w = 2$. This attenuation is consistent with theoretical predictions that benign overfitting may not fully materialize under Cox partial likelihood \citep{liu2025understanding}, where the ranking-based loss geometry differs from squared error.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig1_double_descent.pdf}
\caption{Double descent in deep survival models. Test concordance index versus model width under Scenario A (Gaussian covariates, 30\% censoring). The interpolation threshold (dashed line) marks where parameter count approaches sample size. Test concordance dips sharply at $w = 16$ (star) then recovers in the overparameterized regime, demonstrating the double descent phenomenon in survival analysis.}
\label{fig:double_descent}
\end{figure}

\subsection{Metric Divergence}

Figure~\ref{fig:metric_divergence} displays concordance index (left axis, blue) and integrated Brier score (right axis, red) on the same plot, revealing their divergent behavior across model capacity. Concordance follows the double descent pattern described above: dropping from $0.828 \pm 0.015$ to $0.737 \pm 0.027$ at $w = 16$, then recovering to $0.804 \pm 0.015$ at $w = 2048$.

The integrated Brier score tells a different story. Test IBS increases from $0.469 \pm 0.053$ at $w = 2$ to plateau at approximately $0.523 \pm 0.020$ for $w \geq 16$, and critically, it does not recover in the overparameterized regime. This plateau represents numerical saturation: when survival predictions become extreme (approaching 0 or 1), IBS converges to a constant value regardless of model capacity. The purple dashed line marks the width of maximum divergence between the two metrics.

The key finding is that concordance recovers while IBS saturates. At $w = 2048$, concordance returns to within 0.024 of its baseline value, whereas IBS remains at its saturated level. This behavior reflects the ranking-based nature of Cox partial likelihood optimization: models achieve high concordance through correct risk orderings even as survival probability estimates become degenerate.

The practical implication is direct: a practitioner selecting models by concordance alone would observe acceptable discrimination across the capacity range, potentially selecting a model with uninformative survival probability estimates. The IBS saturation reveals calibration breakdown that concordance misses entirely.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig2_metric_divergence.pdf}
\caption{Discrimination versus calibration across model capacity. Concordance index (blue, left axis) and integrated Brier score (red, right axis) diverge in the overparameterized regime. While concordance recovers beyond $w = 64$, IBS remains elevated, indicating that discrimination-based model selection may miss calibration failures. The purple dashed line marks maximum metric divergence.}
\label{fig:metric_divergence}
\end{figure}

\subsection{Effect of Censoring Rate}

Figure~\ref{fig:censoring} compares the double descent curve under baseline (30\%) and high (90\%) censoring conditions. Under high censoring, only approximately 60 events occur in the training set ($N_{\text{events}} = 0.1 \times 600 = 60$), compared to 420 events under baseline conditions.

The high censoring curve (red) exhibits markedly different behavior from the baseline. Test concordance under high censoring starts at $0.751 \pm 0.111$ at $w = 2$ and improves monotonically to $0.802 \pm 0.024$ at $w = 2048$, without exhibiting a clear double descent pattern. The wide error bands (particularly at small widths) reflect the high variance inherent in extreme censoring scenarios where signal is sparse.

The absence of a pronounced double descent under 90\% censoring has two possible interpretations. The reduced event count may shift the interpolation threshold below our minimum tested width, such that all configurations lie in the overparameterized regime. Alternatively, the noise from extreme censoring may obscure the phenomenon entirely. Both explanations are consistent with the hypothesis that effective sample size (event count) governs the threshold location.

The practical implication is that survival models trained on rare-event data require different model selection strategies. The high variance across seeds under extreme censoring underscores the importance of replicated experiments when assessing neural survival models in low-event settings.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig3_censoring_comparison.pdf}
\caption{Effect of censoring rate on double descent. Test concordance versus model width under 30\% censoring (blue, $N_{\text{events}} \approx 420$) and 90\% censoring (red, $N_{\text{events}} \approx 60$). Under high censoring, the double descent pattern is obscured by noise and high variance across seeds. Shaded regions represent one standard deviation. Stars mark minimum concordance for each scenario.}
\label{fig:censoring}
\end{figure}

\subsection{Regularization Mitigates Double Descent}

Figure~\ref{fig:regularization} compares test concordance across model widths with and without L2 regularization (weight decay $\lambda = 0.01$). Regularization attenuates the double descent phenomenon, though its effects are nuanced.

The unregularized baseline exhibits a sharp performance dip near the interpolation threshold ($w = 16$), with concordance dropping to $0.737 \pm 0.027$. With weight decay, the minimum concordance occurs at $w = 16$ with value $0.755 \pm 0.024$---an improvement of approximately 2\% absolute at the worst point. The regularized curve exhibits a flatter profile across the critical region ($w \in [16, 64]$), suggesting that L2 regularization smooths the transition between underparameterized and overparameterized regimes.

In the overparameterized regime ($w \geq 128$), both curves stabilize, with regularization maintaining a modest but consistent advantage. The shaded region in Figure~\ref{fig:regularization} highlights widths where regularization improves performance. Notably, the regularized models achieve $0.784 \pm 0.016$ at $w = 2048$, compared to $0.804 \pm 0.015$ for unregularized models. This suggests that while regularization helps near the threshold, very large unregularized models may ultimately achieve slightly better discrimination through implicit regularization.

These results partially align with theoretical accounts of double descent \citep{belkin2019reconciling}. Explicit regularization provides benefits near the interpolation threshold, but does not fully substitute for the implicit regularization conferred by extreme overparameterization.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig4_regularization.pdf}
\caption{Effect of L2 regularization on double descent. Test concordance versus model width without regularization (red) and with weight decay $\lambda = 0.01$ (green). Regularization attenuates the performance dip near the interpolation threshold and provides consistent improvement across most model sizes. The shaded region indicates widths where regularization improves concordance. Stars mark minimum concordance for each condition.}
\label{fig:regularization}
\end{figure}

\subsection{Covariate Distribution Effects}

Figure~\ref{fig:scenario_comparison} presents the double descent curve across three covariate types: Gaussian (Scenario A), log-normal (Scenario B), and high-cardinality categorical (Scenario C). All scenarios use 30\% censoring to isolate the effect of covariate distribution.

The Gaussian baseline exhibits the characteristic double descent pattern with a minimum concordance of $0.737 \pm 0.027$ at $w = 16$, recovering to $0.804 \pm 0.015$ at $w = 2048$. Log-normal covariates produce a similar pattern with a minimum of $0.804 \pm 0.039$ at $w = 16$, but with notably stronger recovery: concordance reaches $0.878 \pm 0.030$ in the overparameterized regime, exceeding the Gaussian baseline. The heavy-tailed distribution appears to provide richer signal that larger models can exploit.

The categorical scenario (100 levels per feature, 5 features) yields concordance near 0.52 across all model widths, essentially random performance. This negative result is interpretable: one-hot encoding of 500 categorical levels creates an input dimensionality that far exceeds the sample size, placing even the smallest models in the overparameterized regime with insufficient signal for learning. The categorical scenario thus does not test double descent but rather demonstrates a regime where the learning problem is fundamentally ill-posed.

For Gaussian and log-normal covariates, double descent in survival analysis is robust to covariate distribution, though threshold location and recovery magnitude vary with data characteristics.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig5_scenario_comparison.pdf}
\caption{Double descent across covariate types. Test concordance versus model width for Gaussian (left), log-normal (center), and categorical (right) covariates. All scenarios use 30\% censoring. Stars mark minimum concordance. Gaussian and log-normal covariates exhibit clear double descent patterns, while high-cardinality categoricals yield near-random performance due to extreme input dimensionality. Shaded regions represent one standard deviation across five seeds.}
\label{fig:scenario_comparison}
\end{figure}

% ----------------------------------------------------------------------------
% 5. DISCUSSION
% ----------------------------------------------------------------------------
\section{Discussion}

Our experiments confirm that double descent occurs in survival analysis under Cox partial likelihood training, extending prior observations from classification and regression settings to time-to-event modeling. The phenomenon appears robustly in concordance index across different covariate distributions (Gaussian, log-normal), though its manifestation differs from classification in important ways. Most notably, the integrated Brier score does not exhibit double descent but rather saturates at a constant value in the overparameterized regime, revealing a fundamental decoupling between discrimination and calibration metrics under Cox optimization.

\subsection{Relation to Theoretical Predictions}

\citet{liu2025understanding} provide theoretical analysis of double descent in survival models, and our empirical findings both confirm and extend their predictions. Their analysis predicts that double descent should occur under Cox partial likelihood, which our experiments verify: test concordance exhibits the characteristic non-monotonic pattern with a minimum at the interpolation threshold.

Two of their key predictions align with our observations. First, they suggest that the second descent may be attenuated relative to classification settings because the Cox loss optimizes rankings rather than point predictions. Our results confirm this: concordance recovers from 0.737 to 0.804 but does not return to the baseline of 0.828, a recovery of approximately 74\% rather than 100\%. Second, they note that benign overfitting may not fully materialize under survival losses. The incomplete recovery we observe is consistent with this prediction, though the mechanism differs from their theoretical account: we find that calibration (as measured by IBS) fails entirely in the overparameterized regime, even as discrimination recovers.

One observation extends beyond their theoretical framework. \citet{liu2025understanding} do not distinguish between discrimination and calibration metrics, treating generalization error as a unified quantity. Our experiments reveal that these metrics decouple dramatically: concordance follows a double descent curve while IBS saturates. This decoupling is a direct consequence of the ranking-based Cox loss, which can achieve high discrimination through correct orderings even when the underlying risk scores (and derived survival probabilities) become degenerate. This finding has practical implications not addressed in the theoretical analysis: concordance-based model selection, standard in survival analysis, may mask calibration failures that affect clinical decision-making.

\subsection{Clinical Model Selection}

The double descent curve presents a practical challenge for survival model selection in clinical applications. The interpolation threshold---where test performance is worst---corresponds to models of moderate complexity that might otherwise seem reasonable choices. Our results suggest several strategies for practitioners:

\textit{Avoid the threshold region.} When training neural survival models, practitioners should either (1) constrain capacity to remain clearly underparameterized, or (2) scale to overparameterized regimes where benign overfitting provides protection. The intermediate region, particularly near $P \approx N_{\text{events}}$, should be avoided or traversed quickly during hyperparameter search.

\textit{Account for censoring.} In high-censoring scenarios common to many clinical applications (e.g., rare adverse events, long-term outcomes), the effective sample size is determined by event counts, not total observations. Our experiments with 90\% censoring showed substantially higher variance and an obscured double descent pattern, underscoring the importance of replicated experiments and larger sample sizes when events are rare.

\textit{Use regularization.} L2 regularization via weight decay provides a practical mitigation strategy, attenuating the performance dip without requiring models to be scaled to extreme sizes. A modest weight decay ($\lambda = 0.01$) improved worst-case performance by approximately 2\% absolute concordance in our experiments.

\subsection{Limitations}

Our experiments vary network width while holding depth fixed at two hidden layers. Prior work in classification suggests that width and depth produce similar double descent curves when plotted against total parameter count \citep{nakkiran2021deep}, but whether this equivalence holds under Cox partial likelihood optimization remains untested. Deeper networks introduce additional optimization challenges, including vanishing gradients and the potential need for residual connections, which could interact with the survival loss geometry in ways our experiments do not address.

The categorical covariate scenario (Scenario C) failed to produce meaningful predictions, with concordance near 0.52 across all model sizes. This reflects a fundamental limitation of our experimental design: one-hot encoding of 500 categorical levels creates input dimensionality that overwhelms the sample size. Future work should explore embedding-based approaches for high-cardinality categoricals, which may enable meaningful double descent analysis in such settings.

Our IBS analysis revealed numerical saturation rather than true calibration measurement in the overparameterized regime. When survival predictions become extreme (near 0 or 1 for all subjects), IBS converges to a constant regardless of actual calibration quality. This limits our ability to assess calibration in precisely the regime where it matters most. Alternative calibration metrics that are robust to extreme predictions warrant investigation.

% ----------------------------------------------------------------------------
% 6. CONCLUDING REMARKS
% ----------------------------------------------------------------------------
\section{Concluding Remarks}

We have demonstrated that double descent---the non-monotonic relationship between model capacity and test error---occurs in neural survival models trained with Cox partial likelihood. The phenomenon manifests clearly in concordance index: test concordance drops from 0.828 to 0.737 at the interpolation threshold before recovering to 0.804 in the overparameterized regime. The integrated Brier score, however, saturates at a constant value beyond the threshold, revealing that discrimination and calibration metrics decouple under Cox optimization.

Our experiments with 90\% censoring showed that extreme censoring obscures the double descent pattern and introduces substantial variance, highlighting the importance of event count in determining model behavior. L2 regularization provides effective mitigation at the threshold, improving worst-case concordance by approximately 2\% absolute.

These findings have immediate practical relevance for clinical prognostic modeling, where neural networks are increasingly applied to survival endpoints. The double descent curve implies that moderate-complexity models---often the default in applied settings---may perform worse than either simpler or substantially larger alternatives. Practitioners should use explicit regularization, avoid the interpolation threshold region during hyperparameter search, and recognize that concordance-based model selection may miss calibration breakdown in overparameterized models.

% ----------------------------------------------------------------------------
% SUPPLEMENTARY MATERIALS
% ----------------------------------------------------------------------------
\section*{Supplementary Materials}

Appendix A derives the inverse transform for Weibull-Cox event times. Appendix B details the Gaussian copula procedure for correlated categoricals. Code and data generation scripts are available at [repository URL].

% ----------------------------------------------------------------------------
% ACKNOWLEDGMENTS
% ----------------------------------------------------------------------------
\section*{Acknowledgments}

[To be added]

% ----------------------------------------------------------------------------
% REFERENCES
% ----------------------------------------------------------------------------
\bibliographystyle{apalike}
\bibliography{references}

% ----------------------------------------------------------------------------
% APPENDIX
% ----------------------------------------------------------------------------
\appendix

\section{Inverse Transform for Weibull-Cox Model}

Hazard: $h(t | \bx) = \lambda \nu t^{\nu-1} \exp(\bbeta^\top \bx)$. Cumulative hazard:
\begin{equation}
    H(t | \bx) = \lambda t^\nu \exp(\bbeta^\top \bx).
\end{equation}
Survival function:
\begin{equation}
    S(t | \bx) = \exp\left(-\lambda t^\nu \exp(\bbeta^\top \bx)\right).
\end{equation}
Set $S(T | \bx) = U$, $U \sim \text{Uniform}(0,1)$. Solve:
\begin{align}
    -\lambda T^\nu \exp(\bbeta^\top \bx) &= \ln(U), \\
    T &= \left(\frac{-\ln(U)}{\lambda \exp(\bbeta^\top \bx)}\right)^{1/\nu}.
\end{align}

\section{Gaussian Copula for Correlated Categoricals}

Generate $(Z_1, Z_2)^\top \sim \N(\bm{0}, \bm{\Sigma})$ with off-diagonal $\rho$. Set $X_{\text{cont}} = Z_1$. Compute $U_2 = \Phi(Z_2)$. Define cutoffs $q_0, \ldots, q_K$ from target marginal probabilities. Assign $X_{\text{cat}} = k$ when $q_{k-1} \leq U_2 < q_k$. Rank correlation from the copula carries through; marginals match specification.

\end{document}

\documentclass[12pt,a4paper]{article}

% ============================================================================
% PACKAGES
% ============================================================================

% Typography and Layout
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\doublespacing

% Mathematics
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{bm}

% Graphics and Tables
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{float}

% References and Citations (author-year style for JASA)
\usepackage[authoryear,round]{natbib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

% Supplementary
\usepackage{xcolor}

% ============================================================================
% CUSTOM COMMANDS
% ============================================================================
\newcommand{\cindex}{C\text{-index}}
\newcommand{\ibs}{\text{IBS}}
\newcommand{\nll}{\text{NLL}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btheta}{\bm{\theta}}

% ============================================================================
% TITLE AND AUTHORS
% ============================================================================

\title{The Survival Double Descent: Generalization Dynamics of Deep Neural Networks in Time-to-Event Analysis}

\author{Steven N. Hart, PhD, ACHIP\thanks{Steven N. Hart is Associate Professor, Department of Laboratory Medicine and Pathology, and Department of Quantitative Health Sciences, Mayo Clinic, Rochester, MN 55905 (E-mail: hart.steven@mayo.edu).} \and Ann L. Oberg, PhD\thanks{Ann L. Oberg is Professor, Department of Quantitative Health Sciences, Mayo Clinic, Rochester, MN 55905.}}

\date{}

% ============================================================================
% DOCUMENT
% ============================================================================

\begin{document}

\maketitle

% ----------------------------------------------------------------------------
% ABSTRACT
% ----------------------------------------------------------------------------
\begin{abstract}
Double descent upends classical intuitions about overfitting. Test error drops, spikes at the interpolation threshold, then drops again as models grow larger. The phenomenon is well-documented in classification and regression. Survival analysis is different. Censoring hides true event times. The Cox partial likelihood ranks subjects rather than predicting absolute values. Whether double descent occurs under these conditions, and what form it takes, remains unknown. We construct synthetic survival data with Weibull hazards and controlled censoring to sweep model capacity from underfitting to massive overparameterization. Three questions drive the experiments: Does the interpolation peak shift under high censoring? Does the concordance index miss calibration failures that the integrated Brier score catches? Do log-normal covariates alter the pattern? The answers matter. Clinicians selecting prognostic models often rely on discrimination metrics alone. Our experiments reveal that concordance recovers in the overparameterized regime while integrated Brier score saturates at a constant value regardless of regularization, indicating a consistent decoupling between discrimination and calibration in neural Cox models. This calibration failure persists even with L2 regularization, suggesting it stems from the Cox partial likelihood objective itself rather than optimization pathologies.
\end{abstract}

\clearpage

% ----------------------------------------------------------------------------
% 1. INTRODUCTION
% ----------------------------------------------------------------------------
\section{Introduction}

The bias-variance trade-off has anchored statistical learning theory for decades \citep{hastie2009elements}. Increasing model complexity reduces bias while increasing variance; optimal generalization requires balancing these competing forces. This principle motivated the development of model selection criteria such as cross-validation and regularization methods \citep{tibshirani1996regression}.

Deep learning has challenged this framework. Networks with billions of parameters generalize well despite $p \gg n$ \citep{zhang2021understanding}. \citet{belkin2019reconciling} termed this the double descent phenomenon; \citet{nakkiran2021deep} documented it across architectures and datasets. The pattern proceeds in three stages. Small models underfit. At the interpolation threshold, where capacity just suffices to memorize training data, test error peaks sharply. The model fits noise exactly, but with a unique, highly oscillatory solution. Beyond this threshold, test error decreases again, often falling below the classical minimum. Gradient descent favors minimum-norm solutions when infinitely many interpolants exist \citep{bartlett2020benign}, and this implicit regularization enables generalization in overparameterized models.

Survival analysis remains outside this literature. Neural network methods for time-to-event data, including DeepSurv \citep{katzman2018deepsurv}, DeepHit \citep{lee2018deephit}, and related architectures, now appear routinely in clinical research \citep{wiegrebe2024deep}. However, no systematic investigation has established whether double descent occurs in this setting. \citet{liu2025understanding} provide preliminary theory suggesting it does, though possibly in modified form: the second descent may be attenuated, and benign overfitting, the phenomenon where interpolating models generalize well despite fitting training noise, may not fully materialize.

Several features of survival analysis may alter double descent dynamics. Censoring reduces available information: a patient lost to follow-up contributes only a lower bound on survival time, not a precise measurement. The Cox partial likelihood \citep{cox1972regression} optimizes rankings rather than predictions, and extreme risk scores ($\hat{\eta} \to \pm\infty$) maximize the likelihood for separable data. These properties produce an unusual loss geometry. Whether the interpolation threshold depends on total sample size $N$ or on the number of observed events $N_{\text{events}}$ remains an open question.

Evaluation metrics introduce additional complexity. The concordance index \citep{harrell1996multivariable} measures discrimination, quantifying a model's ability to rank patients by risk. It is invariant to monotonic transformations: a model predicting risk scores of $-1000$ and $+1000$ achieves perfect concordance if the ranking is correct. The integrated Brier score \citep{graf1999assessment} measures calibration and penalizes miscalibrated survival probabilities. These metrics may diverge at the interpolation threshold, with concordance remaining stable while calibration deteriorates. If clinicians select models based on discrimination alone (a common practice; see \citealt{hartman2023pitfalls}), they risk choosing overconfident, unstable predictors.

This paper addresses four questions. First, does double descent occur in deep Cox models? Second, is the interpolation peak governed by $N$ or $N_{\text{events}}$? Third, do discrimination and calibration metrics diverge near the threshold? Fourth, how do skewed covariates and high-cardinality categoricals shift the curve?

We investigate these questions through simulation. Synthetic data permit systematic capacity sweeps, precise noise control, and verification against known ground truth. The experiments span Gaussian and log-normal covariates, low and extreme censoring rates, and categorical features with up to 10 levels. Models are trained without regularization to expose the double descent curve; parallel experiments with weight decay characterize how standard regularization modifies the pattern.

These questions have practical implications. Survival models inform treatment decisions, resource allocation, and patient counseling \citep{harrell2015regression}. A model that discriminates well but is poorly calibrated can produce misleading risk estimates. Understanding the failure modes of neural survival models, and identifying which metrics detect such failures, is necessary for safe clinical deployment.

% ----------------------------------------------------------------------------
% 2. BACKGROUND
% ----------------------------------------------------------------------------
\section{Background}

\subsection{Double Descent Mechanics}

Consider standard regression with target $y = f^*(\bx) + \epsilon$ and noise variance $\sigma^2$. The expected test error decomposes as
\begin{equation}
    \E[(y - \hat{f}(\bx))^2] = \text{Bias}^2[\hat{f}(\bx)] + \text{Var}[\hat{f}(\bx)] + \sigma^2.
\end{equation}
Classical theory predicts that bias decreases with model capacity while variance increases, with optimal complexity achieving the minimum of their sum.

The interpolation threshold disrupts this picture \citep{belkin2019reconciling}. When the number of parameters equals the number of effective constraints, a unique solution interpolates the training data. This solution must pass through every training point, including noise, producing high variance and a peak in test error.

Beyond this threshold, infinitely many interpolating solutions exist. Gradient descent from small initialization converges to the minimum-norm solution \citep{bartlett2020benign}. Smaller weights correspond to smoother functions, and test error decreases.

\subsection{Survival Analysis Setup}

For each subject $i$, we observe $(Y_i, \delta_i)$ where $Y_i = \min(T_i, C_i)$ is the observed time and $\delta_i = \mathbf{1}(T_i \leq C_i)$ is the event indicator. Here $T_i$ denotes the true event time and $C_i$ the censoring time. Censored observations ($\delta_i = 0$) provide only the constraint $T_i > C_i$.

The Cox proportional hazards model \citep{cox1972regression} specifies the hazard function as $h(t | \bx_i) = h_0(t) \exp(\eta_i)$, where $\eta_i = f(\bx_i)$ is the log-risk score and $h_0(t)$ is an unspecified baseline hazard. DeepSurv \citep{katzman2018deepsurv} parameterizes $f$ using a neural network and estimates parameters by maximizing the partial likelihood:
\begin{equation}
    \mathcal{L}(\btheta) = \prod_{i: \delta_i = 1} \frac{\exp(\eta_i)}{\sum_{j \in \mathcal{R}_i} \exp(\eta_j)},
\end{equation}
where $\mathcal{R}_i = \{j : Y_j \geq Y_i\}$ is the risk set at time $Y_i$.

Three properties of the Cox partial likelihood are relevant to double descent \citep{liu2025understanding}. The likelihood depends only on the ranking of risk scores within risk sets, not on their magnitudes. For separable data, the optimal weights diverge to $\pm\infty$. Censored observations contribute to risk sets but not to the likelihood product, reducing the effective sample size.

\subsection{Evaluation Metrics}

The concordance index \citep{harrell1996multivariable} is defined as
\begin{equation}
    C = P(\hat{\eta}_i > \hat{\eta}_j \mid T_i < T_j).
\end{equation}
This is a ranking metric, invariant to monotonic transformations of $\hat{\eta}$. Extreme but correctly-ordered predictions achieve high concordance.

The Brier score at time $t$ \citep{graf1999assessment} is defined as
\begin{equation}
    \text{BS}(t) = \frac{1}{n} \sum_{i=1}^{n} \left[ \hat{S}(t | \bx_i) - \mathbf{1}(Y_i > t) \right]^2 \cdot w_i(t),
\end{equation}
where $w_i(t)$ are inverse probability of censoring weights \citep{gerds2006consistent}. The integrated Brier score averages over a time interval: $\text{IBS} = t_{\max}^{-1} \int_0^{t_{\max}} \text{BS}(t) \, dt$. Unlike concordance (where higher is better), lower IBS indicates better calibration. The Brier score penalizes miscalibrated probability estimates.

At the interpolation threshold, we hypothesize that concordance remains stable because correct rankings may persist even as risk score magnitudes become extreme. The integrated Brier score, however, should increase due to miscalibrated survival curves. Model selection based solely on concordance would fail to detect this deterioration.

% ----------------------------------------------------------------------------
% 3. METHODS
% ----------------------------------------------------------------------------
\section{Methods}

\subsection{Data Generation}

Real datasets lack the controlled conditions required to trace the double descent curve. We therefore generate synthetic survival data.

Covariates are generated using a Gaussian copula. We draw $\bm{Z} \sim \N(\bm{0}, \bm{\Sigma})$ and transform marginals as follows: identity for Gaussian, exponentiation for log-normal, and quantile binning for categorical variables.

Event times follow a Weibull-Cox model:
\begin{equation}
    h(t | \bx_i) = \lambda \nu t^{\nu - 1} \exp(\bbeta^\top \bx_i).
\end{equation}
Inverse transform sampling yields
\begin{equation}
    T_i = \left( \frac{-\ln(U)}{\lambda \exp(\bbeta^\top \bx_i)} \right)^{1/\nu}, \quad U \sim \text{Uniform}(0, 1).
\end{equation}
Ground truth is known exactly \citep{bender2005generating}. The coefficient vector $\bbeta$ contains 10 predictive features with coefficients drawn uniformly from $[-1, 1]$, and 10 noise features with coefficients fixed at zero.

Censoring times follow an exponential distribution with rate $\lambda_c$ calibrated to achieve target censoring proportions. The observed data are $Y_i = \min(T_i, C_i)$ and $\delta_i = \mathbf{1}(T_i \leq C_i)$.

\subsection{Scenarios}

We consider five configurations (Table~\ref{tab:scenarios}). Scenario A uses Gaussian covariates with 30\% censoring as a baseline. Scenario B uses log-normal covariates to test whether leverage points amplify the interpolation peak. Scenario C includes five categorical features with 10 levels each to test threshold shifts from one-hot encoding. Scenario D uses 90\% censoring to test whether the peak location depends on $N_{\text{events}}$ rather than $N$. Scenario E uses nonlinear ground truth with interaction terms ($x_i \cdot x_{i+1}$) and quadratic terms ($x_i^2$) to test whether double descent persists when linear models are insufficient.

\begin{table}[ht]
\centering
\caption{Experimental scenarios.}
\label{tab:scenarios}
\begin{tabular}{llll}
\toprule
\textbf{Scenario} & \textbf{Covariates} & \textbf{Specification} & \textbf{Target} \\
\midrule
A & Gaussian & $X \sim \N(0, I)$, 30\% censoring & Baseline curve \\
B & Log-normal & $X \sim \text{LogNormal}(0, 1)$ & Peak amplitude \\
C & Categorical & 5 features, $K=10$ levels & Threshold location \\
D & Gaussian & 90\% censoring & Effective $N$ \\
E & Gaussian & Nonlinear ground truth & Model complexity \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Models and Training}

The network architecture is a multi-layer perceptron with fixed depth and variable width: two hidden layers, each of width $w$, with ReLU activations, followed by a single linear output node. The model is trained to minimize negative Cox partial log-likelihood.

To observe the double descent phenomenon without confounding effects, we train without explicit regularization: the Adam optimizer runs for 10,000 epochs with batch size 256 and learning rate 0.001, with no early stopping, weight decay, or dropout. Validation data are used to track the epoch with minimum validation IBS for analysis, though all models complete full training. Parallel experiments include weight decay ($\lambda = 0.01$) to characterize how regularization modifies the curve.

All networks use Xavier uniform initialization. The Adam optimizer uses default hyperparameters ($\beta_1 = 0.9$, $\beta_2 = 0.999$, $\epsilon = 10^{-8}$). No gradient clipping, learning rate scheduling, or early stopping is applied. Each configuration trains for exactly 10,000 epochs. Five random seeds (42, 123, 456, 789, 1011) are used for each experimental condition.

We hold depth constant at two hidden layers and sweep width over $w \in \{2, 4, \ldots, 2048\}$ in powers of two, producing parameter counts spanning approximately $0.1N$ to $100N$ for $N = 1000$ samples.

\subsection{Evaluation}

Data are partitioned into training (60\%, $n=600$), validation (20\%, $n=200$), and test (20\%, $n=200$) sets. We compute concordance index, integrated Brier score, and negative log partial likelihood on the held-out test set.

Each configuration is replicated across multiple random seeds. We report means and standard deviations, the location of the interpolation peak, peak magnitude relative to the classical minimum, and the divergence between concordance and integrated Brier score at the threshold.

% ----------------------------------------------------------------------------
% 4. RESULTS
% ----------------------------------------------------------------------------
\section{Results}

\subsection{Baseline Double Descent Curve}

Figure~\ref{fig:double_descent} displays the concordance index as a function of model capacity for Scenario A (Gaussian covariates, 30\% censoring), aggregated across twenty random seeds. Test concordance exhibits a clear double descent pattern: it decreases from $0.756 \pm 0.116$ at $w = 2$ to a minimum of $0.701 \pm 0.046$ at $w = 16$, then recovers to $0.785 \pm 0.037$ at $w = 2048$. The shaded region represents one standard deviation across seeds.

The vertical dashed line marks the interpolation threshold. The test error peak (minimum concordance) occurs at width $w = 16$, corresponding to 625 parameters. This count lies between the training sample size ($N = 600$) and the number of observed events ($N_{\text{events}} \approx 420$ under 30\% censoring). Our data cannot distinguish whether the threshold is governed by $N$ or $N_{\text{events}}$; both explanations are consistent with the observed peak location. The star marker highlights this critical point.

The pattern confirms double descent occurs in survival analysis. The concordance recovery in the overparameterized regime ($w > 64$) is substantial: at $w = 2048$, concordance reaches 0.785, exceeding the baseline of 0.756 observed at $w = 2$. Whether this constitutes complete benign overfitting or an attenuated form, as predicted by \citet{liu2025understanding} for Cox partial likelihood, depends on one's baseline expectation. The recovery to and beyond initial performance suggests the phenomenon is not fundamentally blocked, though the ranking-based loss geometry may still differ from squared error in ways that affect convergence.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig1_main_finding_with_errors.pdf}
\caption{Main findings: calibration failure in neural Cox models. \textbf{(A)} Classical methods (Cox PH, RSF) outperform neural networks on calibration (IBS) despite comparable discrimination (C-index). \textbf{(B)} Discrimination exhibits double descent: test C-index drops from $0.756 \pm 0.116$ to $0.701 \pm 0.046$ at the interpolation threshold ($w = 16$, star), then recovers to $0.785 \pm 0.037$. \textbf{(C)} Calibration fails universally: IBS saturates at approximately $0.52$ for all neural networks regardless of capacity, compared to approximately $0.11$ for Cox PH. \textbf{(D)} Normalized comparison shows discrimination recovers (blue) while calibration does not (red). Shaded regions show $\pm$1 SD across twenty seeds.}
\label{fig:double_descent}
\end{figure}

Table~\ref{tab:baselines} compares neural network performance against classical survival baselines. Cox proportional hazards achieves $C = 0.816 \pm 0.035$ with IBS of $0.113 \pm 0.013$, while the random survival forest achieves $C = 0.773 \pm 0.031$ with IBS of $0.151 \pm 0.006$. The smallest neural network ($w = 2$) achieves $C = 0.756$, substantially below Cox PH ($C = 0.816$), and exhibits worse calibration (IBS $= 0.387$). This calibration gap persists across all neural network sizes, suggesting a limitation of Cox partial likelihood optimization: the ranking-based loss produces well-ordered risk scores but poorly calibrated survival probabilities.

\begin{table}[ht]
\centering
\caption{Comparison of neural networks and classical baselines on Scenario A (Gaussian covariates, 30\% censoring). Values are mean $\pm$ standard deviation across twenty random seeds. Neural networks achieve comparable discrimination to Cox PH but substantially worse calibration as measured by IBS.}
\label{tab:baselines}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{C-index} & \textbf{IBS} \\
\midrule
Cox PH & $0.816 \pm 0.035$ & $0.113 \pm 0.013$ \\
Random Survival Forest & $0.773 \pm 0.031$ & $0.151 \pm 0.006$ \\
\midrule
DeepSurv ($w=2$) & $0.756 \pm 0.116$ & $0.387 \pm 0.124$ \\
DeepSurv ($w=16$, threshold) & $0.701 \pm 0.046$ & $0.523 \pm 0.035$ \\
DeepSurv ($w=2048$) & $0.785 \pm 0.037$ & $0.523 \pm 0.036$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Metric Divergence}

Figure~\ref{fig:double_descent}C--D displays the divergent behavior of concordance and integrated Brier score across model capacity. Concordance follows the double descent pattern described above: dropping from $0.756 \pm 0.116$ to $0.701 \pm 0.046$ at $w = 16$, then recovering to $0.785 \pm 0.037$ at $w = 2048$.

The integrated Brier score tells a different story. Test IBS increases from $0.387 \pm 0.124$ at $w = 2$ to plateau at approximately $0.523 \pm 0.035$ for $w \geq 16$, and critically, it does not recover in the overparameterized regime. Maximum divergence between the two metrics occurs near $w = 2048$, where concordance has recovered while IBS remains saturated.

This IBS plateau appears to reflect a systematic limitation of neural Cox models rather than a numerical artifact. When neural networks optimize Cox partial likelihood, they learn risk scores that correctly rank subjects but do not correspond to well-calibrated hazard ratios. The Breslow estimator, which converts these risk scores to survival probabilities, produces degenerate predictions when the risk score distribution differs substantially from what a proportional hazards model assumes. Classical baselines (Cox PH, RSF) avoid this issue because they estimate baseline hazards jointly with coefficients under appropriate constraints, whereas neural Cox models optimize rankings without such constraints.

The key finding is that concordance recovers while IBS saturates. At $w = 2048$, concordance exceeds the $w = 2$ baseline (0.785 versus 0.756), whereas IBS remains at its saturated level. Critically, L2 regularization does not resolve this decoupling: regularized models achieve IBS of $0.523 \pm 0.036$, essentially identical to unregularized models. This confirms that the calibration failure stems from the Cox partial likelihood objective itself, not from optimization pathologies like exploding weights.

The practical implication is direct: a practitioner selecting models by concordance alone would observe acceptable discrimination across the capacity range, potentially selecting a model with uninformative survival probability estimates. The IBS saturation reveals calibration breakdown that concordance misses entirely.


\subsection{Mechanism of Calibration Failure}

Figure~\ref{fig:mechanism} illustrates why neural Cox models fail at calibration despite achieving reasonable discrimination. Panel A shows that neural networks learn extreme risk scores spanning $[-15, +15]$, while classical Cox PH maintains concentrated scores around zero. Panel B demonstrates that these extreme scores preserve correct rankings across model widths, explaining why C-index remains stable even as risk score distributions change dramatically. Panel C reveals the consequence: the Breslow estimator, which converts risk scores to survival probabilities, produces degenerate predictions (flat or step functions) when risk scores are extreme. Classical Cox PH avoids this failure by jointly estimating baseline hazards with coefficients under appropriate constraints, producing well-calibrated survival curves.

This mechanism explains the calibration-discrimination divergence documented in Figure~\ref{fig:double_descent}. The Cox partial likelihood optimizes only rankings, not magnitudes, allowing networks to achieve high concordance through extreme but correctly-ordered risk scores. These extreme scores then break the Breslow estimator, producing poor calibration regardless of model capacity or regularization.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig2_mechanism.pdf}
\caption{Mechanism of calibration failure in neural Cox models. \textbf{(A)} Risk score distributions: Cox PH produces concentrated, well-behaved risk scores (green), while neural networks learn extreme scores spanning $[-15, +15]$ (purple shows constrained behavior, red shows typical extremes). \textbf{(B)} Extreme risk scores preserve rankings: despite risk score range differences across widths ($2^0$ to $2^{12}$), C-index remains stable because Cox loss only optimizes rankings, not magnitudes. \textbf{(C)} Breslow estimator fails with extreme scores: survival probability estimates become degenerate (flat or step functions) when risk scores are extreme, producing poor calibration regardless of correct rankings. Classical Cox PH (green) produces well-calibrated curves; neural networks at high (red) and low (blue) risk produce uninformative predictions.}
\label{fig:mechanism}
\end{figure}

\subsection{Regularization Mitigates Double Descent}

Figure~\ref{fig:regularization} compares test concordance across model widths with and without L2 regularization (weight decay $\lambda = 0.01$). Regularization attenuates the double descent phenomenon, though its effects are nuanced.

The unregularized baseline exhibits a sharp performance dip near the interpolation threshold ($w = 16$), with concordance dropping to $0.701 \pm 0.046$. With weight decay, the minimum concordance occurs at $w = 16$ with value $0.736 \pm 0.032$, a difference of approximately 3.5\% absolute at the worst point, though this difference does not reach statistical significance given the variance across seeds. The regularized curve exhibits a flatter profile across the threshold region ($w \in [16, 64]$), suggesting that L2 regularization smooths the transition between underparameterized and overparameterized regimes.

In the overparameterized regime ($w \geq 128$), both curves stabilize. The shaded region in Figure~\ref{fig:regularization} highlights widths where regularization improves performance. The regularized models achieve $0.763 \pm 0.042$ at $w = 2048$, compared to $0.785 \pm 0.037$ for unregularized models. This suggests that while regularization helps near the threshold, very large unregularized models may ultimately achieve slightly better discrimination through implicit regularization.

These results partially align with theoretical accounts of double descent \citep{belkin2019reconciling}. Explicit regularization provides benefits near the interpolation threshold, but does not fully substitute for the implicit regularization conferred by extreme overparameterization.

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/fig3_regularization_with_errors.pdf}
\caption{Effect of L2 regularization on double descent. Test concordance versus model width without regularization (red) and with weight decay $\lambda = 0.01$ (green). Regularization attenuates the performance dip near the interpolation threshold and provides consistent improvement across most model sizes. The shaded region indicates widths where regularization improves concordance. Stars mark minimum concordance for each condition.}
\label{fig:regularization}
\end{figure}


% ----------------------------------------------------------------------------
% 5. DISCUSSION
% ----------------------------------------------------------------------------
\section{Discussion}

Our experiments confirm that double descent occurs in survival analysis under Cox partial likelihood training, extending prior observations from classification and regression settings to time-to-event modeling. The phenomenon appears robustly in concordance index across different covariate distributions (Gaussian, log-normal), though its manifestation differs from classification in important ways. Most notably, the integrated Brier score does not exhibit double descent but rather saturates at a constant value in the overparameterized regime, revealing a consistent decoupling between discrimination and calibration metrics under Cox optimization.

\subsection{Relation to Theoretical Predictions}

\citet{liu2025understanding} provide theoretical analysis of double descent in survival models, and our empirical findings both confirm and extend their predictions. Their analysis predicts that double descent should occur under Cox partial likelihood, which our experiments verify: test concordance exhibits the characteristic non-monotonic pattern with a minimum at the interpolation threshold.

Two of their key predictions align with our observations. First, they suggest that the second descent may be attenuated relative to classification settings because the Cox loss optimizes rankings rather than point predictions. Our results show concordance recovers from 0.701 to 0.785, exceeding the baseline of 0.756 observed at $w = 2$. This complete recovery is somewhat surprising given theoretical predictions of attenuation. Second, they note that benign overfitting may not fully materialize under survival losses. While discrimination does recover fully, our experiments reveal a different failure mode: calibration (as measured by IBS) fails entirely in the overparameterized regime, even as discrimination recovers.

One observation extends beyond their theoretical framework. \citet{liu2025understanding} do not distinguish between discrimination and calibration metrics, treating generalization error as a unified quantity. Our experiments reveal that these metrics decouple dramatically: concordance follows a double descent curve while IBS saturates. This decoupling is a direct consequence of the ranking-based Cox loss, which can achieve high discrimination through correct orderings even when the underlying risk scores (and derived survival probabilities) become degenerate. This finding has practical implications not addressed in the theoretical analysis: concordance-based model selection, standard in survival analysis, may mask calibration failures that affect clinical decision-making.

\subsection{Clinical Model Selection}

The double descent curve presents a practical challenge for survival model selection in clinical applications. The interpolation threshold, where test performance is worst, corresponds to models of moderate complexity that might otherwise seem reasonable choices. Our results suggest several strategies for practitioners:

\textit{Avoid the threshold region.} When training neural survival models, practitioners should either (1) constrain capacity to remain clearly underparameterized, or (2) scale to overparameterized regimes where benign overfitting provides protection. The intermediate region, particularly near $P \approx N_{\text{events}}$, should be avoided or traversed quickly during hyperparameter search.

\textit{Account for censoring.} In high-censoring scenarios common to many clinical applications (e.g., rare adverse events, long-term outcomes), the effective sample size is determined by event counts, not total observations. Our experiments with 90\% censoring showed substantially higher variance and an obscured double descent pattern, underscoring the importance of experiments replicated across multiple random seeds and larger sample sizes when events are rare.

\textit{Use regularization for discrimination, not calibration.} L2 regularization via weight decay attenuates the concordance dip near the threshold, improving worst-case discrimination by approximately 3.5\% absolute in our experiments. However, regularization does not resolve calibration failures: IBS remains saturated regardless of weight decay. Practitioners requiring calibrated survival probabilities should consider alternative approaches such as post-hoc recalibration or direct probability prediction methods (e.g., DeepHit).

\subsection{Limitations}

Several limitations constrain the interpretation of our findings. All neural networks used identical optimization hyperparameters (learning rate 0.001, Adam optimizer) regardless of model size. Larger models may benefit from different learning rates or longer training; the elevated negative log-likelihood values observed at higher widths could reflect optimization difficulty rather than statistical overfitting alone. Learning rate scaling with model width, as explored in recent deep learning theory, was not investigated.

Our experiments vary network width while holding depth fixed at two hidden layers. Prior work in classification suggests that width and depth produce similar double descent curves when plotted against total parameter count \citep{nakkiran2021deep}, but whether this equivalence holds under Cox partial likelihood optimization remains untested. Deeper networks introduce additional optimization challenges, including vanishing gradients and the potential need for residual connections, which could interact with the survival loss geometry in ways our experiments do not address.

With twenty random seeds per configuration, the reported standard deviations are substantial (0.031--0.116 for concordance), limiting statistical power to detect small effects and reflecting high variability in individual runs. The claimed regularization benefit of approximately 3.5\% absolute concordance improvement does not reach statistical significance at $\alpha = 0.05$ based on paired testing across seeds.

The categorical covariate scenario (Scenario C) uses 10 levels per feature with one-hot encoding. With 1000 samples distributed across 10 levels per feature, each category appears approximately 100 times on average, providing sufficient observations for learning category-specific hazard effects. This configuration is more representative of clinically realistic scenarios such as disease stage or treatment type.

To address concerns that our linear ground truth may represent a ``straw man'' for neural networks, we conducted additional experiments with nonlinear data-generating processes. These experiments included interaction terms ($x_i \cdot x_j$ for adjacent predictive features) and quadratic terms ($x_i^2$) in the true hazard function, creating relationships that linear models cannot perfectly capture. The double descent pattern persists under nonlinear ground truth: concordance drops from $0.804 \pm 0.023$ at $w = 2$ to $0.740 \pm 0.036$ at $w = 16$, then recovers to $0.809 \pm 0.026$ at $w = 2048$. Extended experiments at $w = 4096$ and $w = 8192$ confirm that recovery plateaus at approximately $C = 0.81$, matching the underparameterized baseline. Interestingly, Cox PH still achieves strong discrimination ($C = 0.82$) on this nonlinear data, suggesting the linear component dominates despite the added nonlinearity. The IBS saturation phenomenon also persists, confirming that calibration failure is intrinsic to Cox partial likelihood optimization regardless of ground truth complexity.

The integrated Brier score for neural networks (approximately 0.52) substantially exceeds that of classical baselines (Cox PH: 0.113, RSF: 0.151), as shown in Table~\ref{tab:baselines}. This gap indicates that survival probability calibration is consistently impaired by Cox partial likelihood optimization, independent of model capacity. Critically, L2 regularization does not resolve this issue: regularized models at $w = 2048$ achieve IBS of $0.522 \pm 0.021$, essentially identical to unregularized models ($0.523 \pm 0.020$). The IBS saturation therefore reflects a systematic limitation of Breslow-based survival probability estimation from Cox models rather than a numerical artifact of exploding weights. Alternative calibration methods, such as post-hoc recalibration or direct survival probability prediction (as in DeepHit), warrant investigation.

% ----------------------------------------------------------------------------
% 6. CONCLUDING REMARKS
% ----------------------------------------------------------------------------
\section{Concluding Remarks}

We have demonstrated that double descent, the non-monotonic relationship between model capacity and test error, occurs in neural survival models trained with Cox partial likelihood. The phenomenon manifests clearly in concordance index: test concordance drops from 0.756 to 0.701 at the interpolation threshold before recovering to 0.785 in the overparameterized regime. The integrated Brier score, however, saturates at approximately 0.52 beyond the threshold regardless of model capacity or regularization, revealing a consistent decoupling between discrimination and calibration under Cox optimization.

This calibration failure is the central finding of our study. Neural Cox models achieve discrimination comparable to classical baselines (C-index within 0.05 of Cox PH) but exhibit calibration 4--5 times worse (IBS of 0.52 versus 0.11--0.15). The persistence of this gap under L2 regularization indicates it stems from the Cox partial likelihood objective itself, not from optimization artifacts. Practitioners requiring calibrated survival probabilities should consider alternatives to neural Cox models, such as direct probability prediction methods.

Our experiments with 90\% censoring showed that extreme censoring obscures the double descent pattern and introduces substantial variance, highlighting the importance of event count in determining model behavior. L2 regularization shows a trend toward improved worst-case concordance (approximately 3.5\% absolute, though not statistically significant) but does not address calibration.

These findings have immediate practical relevance for clinical prognostic modeling. The double descent curve implies that moderate-complexity models may perform worse than either simpler or substantially larger alternatives. More critically, concordance-based model selection (standard practice in survival analysis) will miss the calibration breakdown that affects all neural Cox models regardless of architecture or regularization.

% ----------------------------------------------------------------------------
% SUPPLEMENTARY MATERIALS
% ----------------------------------------------------------------------------
\section*{Supplementary Materials}

Appendix A derives the inverse transform for Weibull-Cox event times. Appendix B details the Gaussian copula procedure for correlated categoricals. Code and data generation scripts are available at \url{https://github.com/Steven-N-Hart/DoubleDescent}.

% ----------------------------------------------------------------------------
% ACKNOWLEDGMENTS
% ----------------------------------------------------------------------------
\section*{Acknowledgments}

[To be added]

% ----------------------------------------------------------------------------
% REFERENCES
% ----------------------------------------------------------------------------
\bibliographystyle{apalike}
\bibliography{references}

% ----------------------------------------------------------------------------
% APPENDIX
% ----------------------------------------------------------------------------
\appendix

\section{Inverse Transform for Weibull-Cox Model}

Hazard: $h(t | \bx) = \lambda \nu t^{\nu-1} \exp(\bbeta^\top \bx)$. Cumulative hazard:
\begin{equation}
    H(t | \bx) = \lambda t^\nu \exp(\bbeta^\top \bx).
\end{equation}
Survival function:
\begin{equation}
    S(t | \bx) = \exp\left(-\lambda t^\nu \exp(\bbeta^\top \bx)\right).
\end{equation}
Set $S(T | \bx) = U$, $U \sim \text{Uniform}(0,1)$. Solve:
\begin{align}
    -\lambda T^\nu \exp(\bbeta^\top \bx) &= \ln(U), \\
    T &= \left(\frac{-\ln(U)}{\lambda \exp(\bbeta^\top \bx)}\right)^{1/\nu}.
\end{align}

\section{Gaussian Copula for Correlated Categoricals}

Generate $(Z_1, Z_2)^\top \sim \N(\bm{0}, \bm{\Sigma})$ with off-diagonal $\rho$. Set $X_{\text{cont}} = Z_1$. Compute $U_2 = \Phi(Z_2)$. Define cutoffs $q_0, \ldots, q_K$ from target marginal probabilities. Assign $X_{\text{cat}} = k$ when $q_{k-1} \leq U_2 < q_k$. Rank correlation from the copula carries through; marginals match specification.

\end{document}
